{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BIO_preprocess.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oAGIh57Tdti","executionInfo":{"status":"ok","timestamp":1657852916403,"user_tz":-420,"elapsed":21279,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}},"outputId":"4d9e1bae-e69c-47a8-8965-701d198a2a65"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Helper"],"metadata":{"id":"AGz37_fATtJc"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"fhjYHs0dTQqJ","executionInfo":{"status":"ok","timestamp":1657852916404,"user_tz":-420,"elapsed":14,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}}},"outputs":[],"source":["import string\n","import re\n","\n","\n","def remove_hex(text):\n","    \"\"\"\n","    Remove Hex\n","    Example: \n","    \"\\xe3\\x80\\x90Ramadan\\xe3\\x80\\x91Dompet wanita multi-fungsi gesper dompet multi-card\"\n","    \"\"\"\n","    res = []\n","    i = 0\n","    while i < len(text):\n","        if text[i] == \"\\\\\" and i + 1 < len(text) and text[i + 1] == \"x\":\n","            i += 3\n","            res.append(\" \")\n","        else:\n","            res.append(text[i])\n","        i += 1\n","    return \"\".join(res)\n","\n","\n","def remove_multiple_whitespace(text):\n","    \"\"\"\n","    remove multiple whitespace\n","    it covers tabs and newlines also\n","    \"\"\"\n","    return re.sub(' +', ' ', text.replace('\\n', ' ').replace('\\t', ' ')).strip()\n","\n","\n","def remove_punctuation(text):\n","    \"\"\"\n","    Removing punctuations\n","    \"\"\"\n","    return re.sub(r'[^\\w\\s]', r' ', text)\n","\n","\n","def remove_space_between_quantity(text):\n","    \"\"\"\n","    200 ml -> 200ml\n","    3 kg -> 3kg\n","    200 x 200 -> 200x200\n","    3 in 1 -> 3in1\n","    Example: \"Double Tape DOUBLE FOAM TAPE 55 mm 45 m 45 makan   2000 x 2000 scs\"\n","    \"\"\"\n","    text = re.sub(r\"([1-9][0-9]*)(in|inch|INCH|Inch|In)( |$)\", r'\\1inch ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(m|meter|M|METER|Meter)( |$)\", r'\\1m ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(mm|milimeter|MM|MILIMETER|Mm)( |$)\", r'\\1mm ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(cm|centimeter|CENTIMETER|CM|Cm)( |$)\", r'\\1ccm ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(pc|pcs|potong|pasang|Pasang|PCS|PC|Pc|Pcs)( |$)\", r'\\1pcs ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(y|year|thn|tahun|Year|Tahun)( |$)\", r'\\1tahun ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(k|kilo|Kilo|kg|kilogram|KG|Kg|Kilogram)( |$)\", r'\\1kg ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(g|gr|gram|G|Gr|GR|GRAM|Gram)( |$)\", r'\\1gr ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(l|liter|L|Liter|LITER)( |$)\", r'\\1l ', text)\n","    text = re.sub(r\"([1-9][0-9]*)(ml|mililiter|ML|mL|Ml)( |$)\", r'\\1ml ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (in|inch|INCH|Inch|In)( |$)\", r'\\1inch ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (m|meter|M|METER|Meter)( |$)\", r'\\1m ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (mm|milimeter|MM|MILIMETER|Mm)( |$)\", r'\\1mm ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (cm|centimeter|CENTIMETER|CM|Cm)( |$)\", r'\\1ccm ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (pc|pcs|potong|pasang|Pasang|PCS|PC|Pc|Pcs)( |$)\", r'\\1pcs ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (y|year|thn|tahun|Year|Tahun)( |$)\", r'\\1tahun ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (k|kilo|Kilo|kg|kilogram|KG|Kg|Kilogram)( |$)\", r'\\1kg ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (g|gr|gram|G|Gr|GR|GRAM|Gram)( |$)\", r'\\1gr ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (l|liter|L|Liter|LITER)( |$)\", r'\\1l ', text)\n","    text = re.sub(r\"([1-9][0-9]*) (ml|mililiter|ML|mL|Ml)( |$)\", r'\\1ml ', text)\n","\n","    text = re.sub(r\"([1-9][0-9]*) (yard|set|lembar|tablet|kaplet|buah|box|sachet|pasang|gb|watt)( |$)\", r'\\1\\2 ', text)\n","\n","    text = re.sub(r\"([1-9][0-9]*) (x) ([1-9][0-9]*)\", r'\\1x\\3', text)\n","    text = re.sub(r\"([1-9][0-9]*) (in) ([1-9][0-9]*)\", r'\\1in\\3', text)\n","    return text"]},{"cell_type":"markdown","source":["Raw Utils"],"metadata":{"id":"-6Q3S1BgT1z8"}},{"cell_type":"code","source":["import random\n","import numpy as np\n","import torch\n","\n","\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"],"metadata":{"id":"uxOyQCpCT0xg","executionInfo":{"status":"ok","timestamp":1657852919425,"user_tz":-420,"elapsed":3030,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Preprocess"],"metadata":{"id":"TMrJWuU4T_jg"}},{"cell_type":"code","source":["import os\n","import sys\n","import re\n","\n","# from helper import remove_hex, remove_multiple_whitespace\n","\n","\n","def convert_raw_to_enimex(input_file: str, output_file: str):\n","    with open(input_file, 'r', encoding=\"ascii\", errors='ignore') as f, open(output_file, 'w') as out:\n","        lines = f.readlines()\n","        i = 0\n","        while i < len(lines):\n","            line = lines[i][:-1]\n","            while i < len(lines) and line[-1] != \"\\\"\":\n","                i += 1\n","                if i < len(lines):\n","                    line += lines[i][:-1]\n","            i += 1\n","            # print(i)\n","            line = line[1:-1]\n","            line = line.replace(\"<\", \" <\").replace(\">\", \"> \")\n","            line = line.replace(\"\\n\", \"\")\n","            line = line.replace(\"\t\", \"\")\n","            line = remove_multiple_whitespace(line)\n","            line = remove_hex(line)\n","            line = remove_space_between_quantity(line)\n","            line = \"\\\"\" + line + \"\\\"\"\n","            line += '\\n'\n","            out.write(line)\n","\n","\n","def convert_enimex_to_stanford(input_file: str, output_file: str):\n","    '''\n","    Convert ENAMEX Named-Entity annotated file to Stanford NLP format (token-based)\n","    @Author research.dimas@gmail\n","    ENAMEX example:\n","    Studies on magnesium\\'s mechanism of action in <ENAMEX TYPE=\"plant\">digitalis</ENAMEX> -induced <ENAMEX TYPE=\"disease\">arrhythmias</ENAMEX> .\n","    '''\n","\n","    START_PATTERN = re.compile(r'^(.*?)<ENAMEX$', re.I)\n","    END_SINGLE_PATTERN = re.compile(r'^TYPE=\"(.*?)\">(.*?)</ENAMEX>(.*?)$', re.I)\n","    TYPE_PATTERN = re.compile(r'^TYPE=\"(.*?)\">(.*?)$', re.I)\n","    END_MULTI_PATTERN = re.compile(r'^(.*?)</ENAMEX>(.*?)$', re.I)\n","    EOS_PATTERN = re.compile(r'^([^<>]*)\\.?\t(\\d+)$', re.I)\n","    NON_ENTITY_TYPE = 'O'\n","\n","    def check_and_process_eos(token):\n","        match = re.match(EOS_PATTERN, token)\n","        if match:\n","            out.write(match.group(1) + '\t' + cur_type + '\\n')\n","            out.write('.' + '\t' + cur_type + '\\n')\n","            out.write('\\n')\n","            return True\n","        return False\n","\n","    cur_type = NON_ENTITY_TYPE\n","    # print(infile)\n","    with open(input_file, 'r', encoding=\"ascii\", errors='ignore') as f, open(output_file, 'w') as out:\n","        lines = f.readlines()\n","        i = 0\n","        while i < len(lines):\n","            line = lines[i][:-1]\n","            i += 1\n","            line = remove_multiple_whitespace(line)\n","            for token in line.strip().split(' '):\n","                token = token.strip()\n","                if not token:\n","                    continue\n","\n","                match = re.match(START_PATTERN, token)\n","                if match:\n","                    if match.group(1):\n","                        out.write(match.group(1) + '\t' +\n","                                  NON_ENTITY_TYPE + '\\n')\n","                    continue\n","\n","                match = re.match(END_SINGLE_PATTERN, token)\n","                if match:\n","                    out.write(match.group(2) + '\t' + match.group(1) + '\\n')\n","                    cur_type = NON_ENTITY_TYPE\n","                    if not check_and_process_eos(match.group(3)):\n","                        out.write(match.group(3) + '\t' + cur_type + '\\n')\n","                    continue\n","\n","                match = re.match(TYPE_PATTERN, token)\n","                if match:\n","                    cur_type = match.group(1)\n","                    out.write(match.group(2) + '\t' + cur_type + '\\n')\n","                    continue\n","\n","                match = re.match(END_MULTI_PATTERN, token)\n","                if match:\n","                    out.write(match.group(1) + '\t' + cur_type + '\\n')\n","                    cur_type = NON_ENTITY_TYPE\n","                    if not check_and_process_eos(match.group(2)):\n","                        out.write(match.group(2) + '\t' + cur_type + '\\n')\n","                    continue\n","\n","                if check_and_process_eos(token):\n","                    continue\n","\n","                out.write(token + '\t' + cur_type + '\\n')\n","\n","\n","def convert_stanford_to_bio(input_file: str, output_file: str):\n","    '''\n","    Convert ENAMEX Named-Entity annotated file to Stanford NLP format (token-based)\n","    @Author research.dimas@gmail\n","    ENAMEX example (2 sentences):\n","    Studies on magnesium\\'s mechanism of action in <ENAMEX TYPE=\"plant\">digitalis</ENAMEX> -induced <ENAMEX TYPE=\"disease\">arrhythmias</ENAMEX> .\n","    '''\n","\n","    NON_ENTITY_TYPE = 'O'\n","\n","    cur_type = NON_ENTITY_TYPE\n","    with open(input_file, 'r', encoding=\"ascii\", errors='ignore') as f, open(output_file, 'w') as out:\n","        prev = None\n","        prev_dot = False  # avoid printing double dot\n","        is_last = False\n","        for line in f.readlines():\n","            tokens = line.split('\t')\n","            token, cur_type = tokens[0], tokens[1][:-1]\n","            if not token or token == \"\":\n","                continue\n","\n","            if len(token) > 2 and token[0] == \"\\\"\" and token[-1] == \"\\\"\":\n","                token = token[1:-1]\n","            elif len(token) > 1 and token[0] == \"\\\"\":\n","                token = token[1:]\n","                out.write('\\n')\n","            elif len(token) > 1 and token[-1] == \"\\\"\":\n","                token = token[:-1]\n","\n","            if token == \"\\\"\":\n","                if not prev_dot:\n","                    out.write(\".\" + '\t' + NON_ENTITY_TYPE + '\\n')\n","                    prev_dot = True\n","                    out.write('\\n')\n","                prev = None\n","            else:\n","                token = token.lower()\n","                if token[-1] == \"\\\"\":\n","                    token = token[:-1]\n","                    is_last = True\n","\n","                if cur_type == NON_ENTITY_TYPE:\n","                    out.write(token + '\t' + cur_type + '\\n')\n","                else:\n","                    if not prev:\n","                        out.write(token + '\tB-' + cur_type + '\\n')\n","                    else:\n","                        if prev == cur_type:\n","                            out.write(token + '\tI-' + cur_type + '\\n')\n","                        else:\n","                            out.write(token + '\tB-' + cur_type + '\\n')\n","                prev = cur_type\n","                prev_dot = False\n","\n","                if is_last:\n","                    prev = None\n","                    if not prev_dot:\n","                        out.write(\".\" + '\t' + NON_ENTITY_TYPE + '\\n')\n","                        prev_dot = True\n","                        out.write('\\n')\n","                    is_last = False\n","\n","\n","def filter_bio(input_file: str, output_file: str):\n","    def filter(s):\n","        res = []\n","        for token in s[:-1]:  # unfilter last sentence\n","            word = token.split(\"\t\")[0]\n","            tag = token.split(\"\t\")[1]\n","            word = remove_punctuation(word)\n","            word = remove_multiple_whitespace(word)\n","\n","            if word != \"\":\n","                res.append(word + \"\t\" + tag)\n","        return \"\".join(res)\n","\n","    with open(input_file, 'r', encoding=\"ascii\", errors='ignore') as f, open(output_file, 'w') as out:\n","        l = 0\n","        s = []\n","        for line in f.readlines():\n","            if line[:-1] == \"\":\n","                if l > 3:\n","                    s = filter(s)\n","                    out.write(s)\n","                    out.write(\"\\n\")\n","                l = 0\n","                s = []\n","            else:\n","                l += 1\n","                s.append(line)\n","\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/MyDrive/Rearch_Dimasataset-plant-disease-corpus.txt\"\n","    output_file = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO/enimex.txt\"\n","\n","    convert_raw_to_enimex(input_file=input_file, output_file=output_file)\n","\n","    input_file = output_file\n","    output_file = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO/stanford.txt\"\n","\n","    convert_enimex_to_stanford(input_file=input_file, output_file=output_file)\n","\n","    input_file = output_file\n","    output_file = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO/BIO.txt\"\n","\n","    convert_stanford_to_bio(input_file=input_file, output_file=output_file)\n","\n","    input_file = output_file\n","    output_file = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO/final-data.txt\"\n","\n","    filter_bio(input_file=input_file, output_file=output_file)"],"metadata":{"id":"Zj4eBv1VUAup","executionInfo":{"status":"ok","timestamp":1657852923129,"user_tz":-420,"elapsed":3737,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["! ls -lh /content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCL5EYLdUrXZ","executionInfo":{"status":"ok","timestamp":1657852923139,"user_tz":-420,"elapsed":55,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}},"outputId":"5c5c0d00-0a56-46b0-f9c2-c9a90bb40361"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["total 1.7M\n","-rw------- 1 root root 336K Jul 15 02:39 annotated-dataset-plant-disease-corpus.txt\n","-rw------- 1 root root 344K Jul 15 02:42 BIO.txt\n","-rw------- 1 root root 342K Jul 15 02:42 enimex.txt\n","-rw------- 1 root root 326K Jul 15 02:42 final-data.txt\n","-rw------- 1 root root 393K Jul 15 02:42 stanford.txt\n"]}]}]}