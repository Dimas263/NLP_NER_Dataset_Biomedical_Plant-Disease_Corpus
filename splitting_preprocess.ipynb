{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"splitting_preprocess.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"we0uMPdxX1yg","executionInfo":{"status":"ok","timestamp":1657853457615,"user_tz":-420,"elapsed":4591,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}},"outputId":"50964f8e-d691-4612-8aaa-0d4a7152a1d6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# **Splitting Raw Data**"],"metadata":{"id":"WKb-SetiYOYQ"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"mNHSuNZGXrIA","executionInfo":{"status":"ok","timestamp":1657853457618,"user_tz":-420,"elapsed":36,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}}},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","\n","# from raw_utils import set_seed\n","\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","\n","def save_data(data, fpath):\n","    with open(fpath, 'w') as out:\n","        for instance in data:\n","            for token in instance:\n","                out.write(token)\n","            out.write(\"\\n\")\n","\n","\n","if __name__ == \"__main__\":\n","\n","    set_seed(26092020)\n","\n","    fpath = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/BIO/final-data.txt\"\n","\n","    file = open(fpath)\n","    lines = file.readlines()\n","    file.close()\n","\n","    data = []\n","    instance = []\n","\n","    for l in lines:\n","        if l[:-1] == \"\":  # if it's empty\n","            data.append(instance)\n","            instance = []\n","        else:\n","            instance.append(l)\n","\n","    random.shuffle(data)\n","\n","    train_size = int(0.6 * len(data))\n","\n","    train_fpath = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/raw_data/train.txt\"\n","    test_fpath = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/raw_data/test.txt\"\n","    dev_fpath = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/raw_data/dev.txt\"\n","\n","    save_data(data[:train_size], train_fpath)\n","    save_data(data[train_size:], test_fpath)\n","    save_data(data[train_size + 24:], dev_fpath)"]},{"cell_type":"code","source":["! ls -lh /content/drive/MyDrive/Rearch_Dimas/NER-DATASET/raw_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPZtleY-YMW6","executionInfo":{"status":"ok","timestamp":1657853457619,"user_tz":-420,"elapsed":33,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}},"outputId":"6f5c9103-f2bc-4568-ff2d-74bc249170c6"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["total 451K\n","-rw------- 1 root root 125K Jul 15 02:51 dev.txt\n","-rw------- 1 root root 131K Jul 15 02:51 test.txt\n","-rw------- 1 root root 195K Jul 15 02:51 train.txt\n"]}]},{"cell_type":"markdown","source":["# **Splitting Mid Data**"],"metadata":{"id":"TQaRLMOvYfAh"}},{"cell_type":"code","source":["import os\n","import re\n","import json\n","\n","root_path = \"/content/drive/MyDrive/Rearch_Dimas/NER-DATASET/\"\n","\n","def preprocess(input_path, save_path, mode):\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","    data_path = os.path.join(save_path, mode + \".json\")\n","    labels = set()\n","    result = []\n","    tmp = {}\n","    tmp['id'] = 0\n","    tmp['text'] = ''\n","    tmp['labels'] = []\n","\n","    with open(input_path, 'r', encoding='utf-8') as fp:\n","        lines = fp.readlines()\n","        texts = []\n","        entities = []\n","        words = []\n","        entity_tmp = []\n","        entities_tmp = []\n","        for line in lines:\n","            line = line.strip().split(\"\t\")\n","            if len(line) == 2:\n","                word = line[0]\n","                label = line[1]\n","                words.append(word)\n","\n","                if \"B-\" in label:\n","                    entity_tmp.append(word)\n","                    if (\" \".join(entity_tmp), label.split(\"-\")[-1]) not in entities_tmp:\n","                        entities_tmp.append((\"\".join(entity_tmp), label.split(\"-\")[-1]))\n","                    labels.add(label.split(\"-\")[-1])\n","                    entity_tmp = []\n","\n","                elif \"I-\" in label:\n","                    entity_tmp.append(word)\n","                    if (\" \".join(entity_tmp), label.split(\"-\")[-1]) not in entities_tmp:\n","                        entities_tmp.append((\"\".join(entity_tmp), label.split(\"-\")[-1]))\n","                    entity_tmp = []\n","                    labels.add(label.split(\"-\")[-1])\n","            else:\n","                texts.append(\" \".join(words))\n","                entities.append(entities_tmp)\n","                words = []\n","                entities_tmp = []\n","\n","    i = 0\n","    for text, entity in zip(texts, entities):\n","\n","        if entity:\n","            ltmp = []\n","            for ent, type in entity:\n","                for span in re.finditer(ent, text):\n","                    start = span.start()\n","                    end = span.end()\n","                    ltmp.append((type, start, end, ent))\n","                    # print(ltmp)\n","            ltmp = sorted(ltmp, key=lambda x: (x[1], x[2]))\n","            tmp['id'] = i\n","            tmp['text'] = text\n","            for j in range(len(ltmp)):\n","                tmp['labels'].append([\"T{}\".format(str(j)), ltmp[j][0], ltmp[j][1], ltmp[j][2], ltmp[j][3]])\n","        else:\n","            tmp['id'] = i\n","            tmp['text'] = text\n","            tmp['labels'] = []\n","        result.append(tmp)\n","        # print(i, text, entity, tmp)\n","        tmp = {}\n","        tmp['id'] = 0\n","        tmp['text'] = ''\n","        tmp['labels'] = []\n","        i += 1\n","\n","    with open(data_path, 'w', encoding='utf-8') as fp:\n","        fp.write(json.dumps(result, ensure_ascii=False))\n","\n","    if mode == \"train\":\n","        label_path = os.path.join(save_path, \"labels.json\")\n","        with open(label_path, 'w', encoding='utf-8') as fp:\n","            fp.write(json.dumps(list(labels), ensure_ascii=False))\n","\n","\n","preprocess(root_path + \"raw_data/train.txt\", root_path + \"mid_data\", \"train\")\n","preprocess(root_path + \"raw_data/dev.txt\", root_path + \"mid_data\", \"dev\")\n","preprocess(root_path + \"raw_data/test.txt\", root_path + \"mid_data\", \"test\")\n","\n","labels_path = os.path.join(root_path + \"mid_data/labels.json\")\n","with open(labels_path, 'r') as fp:\n","    labels = json.load(fp)\n","\n","tmp_labels = []\n","tmp_labels.append('O')\n","for label in labels:\n","    tmp_labels.append('B-' + label)\n","    tmp_labels.append('I-' + label)\n","\n","label2id = {}\n","for k, v in enumerate(tmp_labels):\n","    label2id[v] = k\n","path = root_path + \"mid_data/\"\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","with open(os.path.join(path, \"nor_ent2id.json\"), 'w') as fp:\n","    fp.write(json.dumps(label2id, ensure_ascii=False))\n"],"metadata":{"id":"gBcTy93hYiNL","executionInfo":{"status":"ok","timestamp":1657853458197,"user_tz":-420,"elapsed":596,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["! ls -lh /content/drive/MyDrive/Rearch_Dimas/NER-DATASET/mid_data"],"metadata":{"id":"X202bpZ8ZUJa","executionInfo":{"status":"ok","timestamp":1657853458200,"user_tz":-420,"elapsed":21,"user":{"displayName":"Data Mining","userId":"02757270100510414161"}},"outputId":"9b25838d-27bb-451e-e1a9-dcabd3eabdf5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["total 612K\n","-rw------- 1 root root 169K Jul 15 02:51 dev.json\n","-rw------- 1 root root   20 Jul 15 02:51 labels.json\n","-rw------- 1 root root   68 Jul 15 02:51 nor_ent2id.json\n","-rw------- 1 root root 177K Jul 15 02:51 test.json\n","-rw------- 1 root root 265K Jul 15 02:51 train.json\n"]}]}]}